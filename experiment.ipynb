{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from arch.config import Config\n",
    "from arch.model import NanoFormerForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`RotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/datta0/models/nanoformer/ngpt_full_new_1ep/best_model'\n",
    "# model_path = '/home/datta0/models/nanoformer/gqa_minipile_1ep/checkpoint-0'\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    raise OSError(f\"Path {model_path} does not exist\")\n",
    "config_path = os.path.join(model_path, 'config.json')\n",
    "if not os.path.isfile(config_path):\n",
    "    raise OSError(f'Config file does not exist in {model_path}')\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = Config(**json.load(f))\n",
    "    config.gradient_checkpointing = False\n",
    "\n",
    "model = NanoFormerForCausalLM(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.7722,  0.7752, -0.5999,  ...,  2.0298, -0.3548,  0.6717],\n",
       "         [-0.5416,  1.1817,  1.1714,  ...,  0.0441, -0.9582, -0.9147],\n",
       "         [-0.2746, -0.7430,  1.0229,  ..., -1.8631,  0.0672, -0.7881],\n",
       "         ...,\n",
       "         [-0.6127, -0.6503, -0.1991,  ...,  0.2042,  0.6728,  0.8433],\n",
       "         [-0.6183,  0.3486,  1.2508,  ..., -0.5478,  0.7902,  0.5463],\n",
       "         [-0.0797, -0.5059,  0.7187,  ...,  0.4446,  0.8132,  0.4625]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0619, -0.0468, -0.0600,  ...,  0.0579, -0.0264,  0.0656],\n",
       "         [-0.0053, -0.0014, -0.0181,  ..., -0.0016, -0.0706, -0.0135],\n",
       "         [-0.0212,  0.0609,  0.0306,  ..., -0.0723, -0.0439,  0.0407],\n",
       "         ...,\n",
       "         [ 0.0315, -0.0125,  0.0224,  ...,  0.0249,  0.0431,  0.0302],\n",
       "         [ 0.0103, -0.0107, -0.0215,  ...,  0.0274, -0.0368, -0.0064],\n",
       "         [-0.0631,  0.0515, -0.0587,  ..., -0.0079, -0.0129,  0.0227]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight, model.model.layers[0].attention.q.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(model_path, 'pytorch_model.bin'), weights_only=True),strict=False)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NeelNanda/gpt-neox-tokenizer-digits\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "print('Model loaded successfully')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].attention.use_ngpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.embed_tokens.weight, model.model.layers[0].attention.q.weight\n",
    "# model.model.layers[0].attention.sqk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m new_tokens:\n\u001b[1;32m     16\u001b[0m     count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Get the logits for the last token\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     next_token_logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.venvs/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/nanoformer/arch/model.py:103\u001b[0m, in \u001b[0;36mNanoFormerForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, return_loss)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, return_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlogit_cap:\n",
      "File \u001b[0;32m~/.venvs/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/nanoformer/arch/model.py:72\u001b[0m, in \u001b[0;36mNanoFormer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, mask, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_ids\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.venvs/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.venvs/pyenv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/pyenv/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  # Import for softmax\n",
    "\n",
    "with torch.no_grad():\n",
    "    text = \"Be afraid, England and Wales 2019. The Aussies are coming. Or rather, the Aussies are still coming, after an 86-run defeat of a New Zealand team who seemed consumed by the occasion at Lord’s. At times in the Black Caps’ attempts to chase 243 this felt a bit like a Sunday morning junior age group game. Steve Smith sent down some weird, wonky all-sorts. Wickets were greeted with jokey huddles. It took the return of Mitchell Starc to restore a sense of World Cup order, figures of five for 26 reflecting a spell of brutal, high-grade, white-ball fast-bowling that blew away the tail. Pakistan’s Imad Wasim holds nerve to see off Afghanistan in thriller Read more Victory leaves Australia on their own at the top of the group stage table with seven wins from eight, and with some of their own question marks finding an answer or two. They had some help along the way, not least from Kane Williamson’s diffident captaincy. On a sun-baked north London day New Zealand had first shown how to beat Australia; then almost immediately they showed how to fail to beat Australia. Exposing that thin-looking middle order had always looked a plan. Failing to punch through by taking off your best bowlers was where the game got away, captured by the sight of the skipper wheeling out seven overs of mid-innings part-time leg-spin. Trent Boult even had time at the end of Australia’s innings to conjure a largely pointless World Cup hat-trick. Instead it was a gutsy, occasionally streaky 107-run sixth-wicket partnership between Usman Khawaja and Alex Carey that decided this game. From the start Lord’s was a place of Trans-Tasman good cheer as the grey shroud of the last few weeks lifted. Australia had won the toss and elected to bat. In any list of David Warner’s top five career sledges\"\n",
    "    tokens = tokenizer([text], return_tensors='pt')\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "\n",
    "    new_tokens = 50  # Set a maximum length for the generated text\n",
    "    temperature = 1e-3 # Adjust this for more or less randomness\n",
    "    top_k = 10 # Adjust this for more or less randomness\n",
    "    count=0\n",
    "    \n",
    "    while count < new_tokens:\n",
    "        count+=1\n",
    "        output = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Get the logits for the last token\n",
    "        next_token_logits = output[0][:, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        if top_k > 0:\n",
    "            values, _ = torch.topk(next_token_logits, top_k)\n",
    "            min_values = values[:, -1].unsqueeze(1).expand_as(next_token_logits)\n",
    "            next_token_logits = torch.where(next_token_logits < min_values,\n",
    "                                            torch.ones_like(next_token_logits) * -1e10,\n",
    "                                            next_token_logits)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        predicted_token_id = torch.multinomial(next_token_probs, num_samples=1)\n",
    "        \n",
    "        # Append the predicted token to the input sequence\n",
    "        input_ids = torch.cat((input_ids, predicted_token_id), dim=1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(1, 1, dtype=attention_mask.dtype, device=attention_mask.device)], dim=1)\n",
    "        \n",
    "        # Decode the current sequence\n",
    "        out_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(tokenizer.decode(input_ids[0][-1], skip_special_tokens=True).replace('Ġ',''), end=' ', flush=True)\n",
    "        text = out_text\n",
    "        \n",
    "        # Check for end-of-sequence token (optional, depends on your tokenizer)\n",
    "        # if predicted_token_id == tokenizer.eos_token_id:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5023,  220,   75,  337,  292,   17, 1671,  248, 3106, 2373,   19,  317,\n",
       "          259,  971,  362,  414,  372,  247,   19, 1035, 3442,   17,  223,  259,\n",
       "          971,  362,  414, 1761,  372,  247,   17,  656,  348,  890,   27,   18,\n",
       "           87,  314, 3541,  236,  220,  594, 2649,  781,  561,  436,  310,  229,\n",
       "          780, 4583,  333,  223, 3974,  251,  343, 3853, 1551,   88,   19, 1030,\n",
       "         2153,  239,  223, 2256, 2554,   88, 1551, 2293, 3321,  254,  422,  642,\n",
       "         1855,   24,  630,  240, 2717,  220,  244,  235, 1800,  220, 4344,  296,\n",
       "          255, 4728,  558, 3436, 2012, 1031, 1187,   19, 4957, 2811, 2174, 1493,\n",
       "          980, 1547, 1125,   17, 1193, 3360,  725,   18,   88, 1352,   19,  316,\n",
       "          718, 1038,  452,  308, 1522,  229,  331,  558,  506,  522,  263, 2558,\n",
       "          578,   19,  519, 1543,  223, 1222,  236,  266,  235,  733,  743, 3269,\n",
       "           72,  254, 1596,  393,  220,  238, 1869,  236,  934, 1480, 1677,   17,\n",
       "         3248,  958,  236, 1635,  299, 2152, 1692,  555,  247,  220,  453,  430,\n",
       "          236, 2062,  306,  232,   17, 1040,   18, 2275,  617,   17, 2865,   18,\n",
       "          571,  240,  394,   18,   71,  311, 1308,  364,  244,  252,   92, 2728,\n",
       "          223,  217,  639,   19, 4060, 3068, 1551,   88, 2928,  275,  316,  249,\n",
       "          303, 2455,   88,  283, 3137,  254, 2270,  679, 1351,  645,  225, 3068,\n",
       "          239, 5072,  373,  221, 1526,  275,  818,  417,  727,  501, 5096, 1724,\n",
       "          300,  553, 1089,  343,  223, 1750,  236,  223, 1031, 2953,  217,  676,\n",
       "          331, 2887,  233,  752,  351, 2758,   17,  248,  331,  980,  236,  553,\n",
       "         1089, 3787,  251, 2527,  420, 2313,  247,  348,  348,   88, 3507,  399,\n",
       "          623,   19, 1327,  504,  980, 2484, 1375,  223, 2164,   17,  511, 3211,\n",
       "          351,  357, 1332, 1546,  893, 1551,   88, 1199,   75,  817, 1120, 1716,\n",
       "         1036,   19,  778,  220,  238,  314,   18,   71,  389,  229, 1556, 1555,\n",
       "         1662,  594, 2649,  504,  516, 4158, 2842,  254,  340,  228, 1724,   32,\n",
       "          900, 3440, 4086, 1573,  698, 1094,  229, 2842,  254, 4779,  254,  340,\n",
       "          228, 1724,   19, 1400,   85, 3694,  364,  294,  219,   18, 1739,  506,\n",
       "          247, 4317, 1677,  504, 4548, 3714,  229,  220, 2972,   19,  307,  639,\n",
       "          247,  254,  246,  314,  304,  867,  333, 3750,  679,  468,  415, 1843,\n",
       "          244,  311,   81,  280,  286,  833,  223, 1187, 4350, 2728,   17, 2891,\n",
       "         1315,  333,  223,  238,  481,  236,  223, 1704,  388,  509,  523,  267,\n",
       "          247,  683, 2887,  230, 1569,  236, 2325,   18,  759,  646,  592,   18,\n",
       "         2551, 1548,   18, 1128,  219,   19,  262, 1286,  272, 2516,   89, 1931,\n",
       "          504,  740,  343,  223,  866,  236, 1724, 1551,   88, 4467,  646,  254,\n",
       "          350,   79,  445,  220, 1733,  672, 2339, 2284,  934, 1480,  263,  228,\n",
       "           18,   89, 1612,   19,  409, 2694,  384,  286,  220,  308,  306, 2662,\n",
       "           17, 3974,  820,  298,  322,  224,  389,   94,  957,   28,   18,   87,\n",
       "          314, 1604,  319,   18, 4919,  278, 5024, 2573,  850,  368,   88,  484,\n",
       "         3322,  557,  789,   70,  248, 2660,  256,  466,   94,  364, 3338,  630,\n",
       "         1187,   19, 2733,  223, 1302, 3853, 1551,   88,  286,  220, 1469,  236,\n",
       "         2624,   18,   57,  249,  484, 2517, 3314,  221,  313,  223,  308, 2525,\n",
       "          464,  250,  383,  236,  223, 1531, 2106, 4270,  274,  375,  653,   19,\n",
       "         1724,  504, 1193,  223,  254,  647,  248, 2473,  254,  244,  228,   19,\n",
       "          409, 1386, 1371,  236, 2081,  869, 1246, 1551,   88, 1750, 1635, 1267,\n",
       "          238, 1177, 2354,  182,  181,  181,  181,  181,  181,  181]),\n",
       " tensor([[181]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0], predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(output[0][0], dim=-1), torch.argmax(output[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(output[0][-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.model.embed_tokens(input_ids)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0).expand(input_ids.size(0), -1)\n",
    "position_embeddings = model.model.rotary_emb(input_ids, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for layer in model.model.layers:\n",
    "        x = layer(x, position_embeddings)\n",
    "        print(x.shape)\n",
    "        print(x)\n",
    "        print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
